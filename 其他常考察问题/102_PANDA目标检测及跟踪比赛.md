# PANDA

脑图链接：https://naotu.baidu.com/file/9ff39dab02a1ba3712cec0b8f5ebfff3

![PANDA检测及跟踪](H:/Application/Typora图片暂存文件夹/PANDA检测及跟踪-1626270037192.svg)

程序的布局思路如下：

初赛的检测框架不用变，直接在检测结果后面添加DeepSORT库即可，（DeepSORT库封装的非常好）

具体的改动位置参考：
检测框架添加DeepSORT的位置[`TianChi_PANDA_round1_2021/ code / Task1_test.py `](https://gitee.com/xn1997/TianChi_PANDA_round1_2021/blob/master/code/Task1_test.py#L344)
DeepSORT框架具体位置[`DeepSORT-self/deep_sort.py`](https://gitee.com/xn1997/deep-sort-self/blob/master/deep_sort.py#L101)

即初赛程序只负责检测，复赛程序负责跟踪+保存提交结果（这里的提交格式与MOT标注格式完全一样）

## 任务

类别：行人可见区域、行人全身区域、行人头部、车辆

推理时间：单张图片推理时间不超过90s

![image-20210716105836906](https://raw.githubusercontent.com/xn1997/picgo/master/image-20210716105836906.png)

## 数据分析

1. 高分辨率：达到10亿像素。
2. 行人的可见区域和全身区域大面积重叠，难以区分。
3. 目标尺度跨度大，且场景拥挤、遮挡严重（小的目标宽高只有几十个像素，大目标宽高为几千）

## 数据处理

1. 将大图切割为小图，进行训练
   1. 原因：图像过大无法训练。
   2. 方法：将大图按照从左到右、从上到下的方式切割为2560×2560的子图，其中重叠部分的像素为子图大小的0.2倍；如果目标保留的部分不足0.5就去除该目标的标签；添加0.5和0.1等多尺度训练效果会更好。
      留有重叠，可以避免目标被分割，影响图像特征的提取。
2. 多进程切图
   1. 每次切完图之后都需要保存切图结果，需要花费几个小时，所以制作训练集的过程很慢。
   2. 使用Python的进程池，开启多个进程分别切割不同的数据集，速度提升了3倍。

<big><font color='orange'>数据增强</font></big>

只使用了翻转，随机选择经过测试会掉点，效果不好

## 推理过程

1. 多尺度预测
   1. 目标尺度跨度太大，尺度小了大目标检测不到，尺度大了小目标检测不到。
   2. 方法：
      1. 图片缩放到为1，0.5，0.1三个尺度，然后按照overlap=0.5的步长统一切割为2560*2560子图，对每个子图进行预测。
         多尺度可以保证，即可以检测到远处的小目标，也可以检测到近处的大目标（因为0.1尺度下，近处的目标就不会被分割开了）
      2. 对于每个patch的预测结果，如果其距离patch边界小于10个像素就直接剔除。
         靠近边缘的预测结果，可以视为当前目标只有部分在该patch内，那么显然根据部分信息得到的预测框必然精度不高，而且overlap=0.5，该目标必然会在另一个overlap里完全出现，不必担心漏检该目标
2. 将所有子图的结果融合到一起进行NMS，使用DIOU替换IOU
   1. IOU无法衡量目标框之间的距离，即如果两个物体IOU比较大，而中心距离比较远时，依然会被滤除
   2. DIOU考虑了中心距离，就避免过滤掉这些目标了，减少了漏检。
3. 这里可以引入投票法和wbf两种替代NMS的方法。
   1. WBF
      1. 原因：模型集成常用算法，一般是两个模型各自NMS之后，进行模型融合使用。
         直接将NMS换成WBF效果不好。
      2. 方法：①根据box之间的IOU值进行聚类，IOU大于0.55的都聚为一类
         ②更新聚类中心：box坐标按置信度conf取加权平均；conf直接取平均。
         ③所有的聚类中心做为结果
      3. 虽然两个模型集成有2个点的提升，但受限于训练时间，就没有使用。

## 网络结构

1. cascade rcnn
   1. 检测效果好
   2. 在mmdetection框架下，相比于FCOS等anchor free的模型，在开启FP16时，训练稳定且可以将batch提高，减少内存，而其他模型FP16会导致loss为nan（初步猜测是因为梯度消失），或者内存减少不明显，因此该模型对于显存较小的机器很友好
2. DCN
   1. 在stage2,3,4的最后添加DCN模块
   2. 可以提取出适应目标形状的特征，相较于传统卷积，提取的特征更能覆盖目标

## 行人特征提取器

使用在naic中改进的特征提取器，基于MGN的改进版。

1. 利用目标跟踪数据集生成行人重识别数据集，进行训练即可。
   思路：

   >  对于每一个视频段
   >  	对于每个ID
   >  		找出其在不同帧中的位置
   >  				切出该目标，并进行命名，同时标注他的id和camid(camid统一置为0为了训练而已，无意义)
   
2. 在检测结果获得之后，不是直接resize，而是先按宽缩放，其余部分均值填充，缩放到(64*128)

   1. 原因：直接resize会导致半身目标无法对齐的问题，宽一般都是可以检测出来的，而高经常因为遮挡检测不到，因此以宽resize，以高等比缩放，其余均值填充，可以缓解一定程度的对齐问题。
   2. 在生成训练集、推理过程中都使用这个策略。

## 数据关联（目标跟踪）

参考DeepSORT算法<u>《201_目标跟踪相关细节（个人笔记使用，无关面试）.md》</u>

1. 使用检测器获取第一帧的检测结果D，并初始化为轨迹。
2. 使用reid网络获取每个行人的特征。
   对图片按照宽进行等比例resize，其余部分均值填充，再输入reid网络。
3. 首先利用目标之间的IOU距离进行第一步的KM匹配，针对后续没有匹配成功的目标，进行后续利用图像特征的二次匹配。
4. 计算当前帧各检测结果的特征与行人历史特征的距离，并取最小的一个距离作为该目标与该行人的距离，距离越近就说明相似度越高。利用该图像信息的距离矩阵进行二次KM匹配以解决遮挡问题。
5. 最后把没有匹配成功的检测框作为新的轨迹。

# 评价指标

<font color='red'><big>初赛</big></font>

**mAP和AR500的调和均值**：即考虑查准率也考虑查全率

<img src="https://raw.githubusercontent.com/xn1997/picgo/master/image-20210714103414256.png" alt="image-20210714103414256" style="zoom:100%;" />

<font color='red'><big>复赛</big></font>

**MOTA和MOTP的调和均值**

![image-20210714121530529](H:/Application/Typora图片暂存文件夹/image-20210714121530529.png)

