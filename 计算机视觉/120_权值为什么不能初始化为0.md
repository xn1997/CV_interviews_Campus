## 问题

权值为什么不能初始化为0

## 解答

![img](https://pic3.zhimg.com/80/v2-5f28bdf596c15467ab7fec93f2c978fa_720w.jpg)

## 逻辑回归(logistic)参数初始化为什么可以为0

**逻辑回归相比神经网络的区别：其就是一个单层的神经网络，不过激活函数是sigmoid函数**

逻辑回归的结构图如下：输入为x1、x2，权值为w11、w21，输出为a1

<img src="https://pic1.zhimg.com/80/v2-09de854e019da9c6fc7dabf1c76042f0_720w.jpg" alt="img" style="zoom:70%;" />

前向传播/交叉熵损失公式为：
$$
\begin{aligned}
激活函数f(x)&=sigmoid(x) \\
无激活函数的输出z_1&=w_{11}x_1+w_{21}x_2+b \\
输出a_1&=f(z_1) \\
损失L&=-ylog(a_1)-(1-y)log(1-a_1)
\end{aligned}
$$
反向传播：
$$
\begin{aligned}
前置条件f'(x)&=f(x)(1-f(x)) \\
即\frac{\partial a_1}{\partial z_1}&=a_1(1-a_1) \\
da_1&=\frac{\partial L}{\partial a_1}=-\frac{y}{a_1}+\frac{1-y}{1-a_1} \\
dw_{11}&=\frac{\partial L}{\partial w_{11}}=\frac{\partial L}{\partial a_1}\frac{\partial a_1}{\partial z_1}\frac{\partial z_1}{\partial w_{11}}=[-\frac{y}{a_1}+\frac{1-y}{1-a_1}][a_1(1-a_1)][x_1]=(a_1-y)x_1 \\
dw_{21}&=\frac{\partial L}{\partial w_{21}}=\frac{\partial L}{\partial a_1}\frac{\partial a_1}{\partial z_1}\frac{\partial z_1}{\partial w_{21}}=(a_1-y)x_2 \\
db&=\frac{\partial L}{\partial b}=\frac{\partial L}{\partial a_1}\frac{\partial a_1}{\partial z_1}\frac{\partial z_1}{\partial b}=(a_1-y) \\
参数更新公式： \\
w_{11}&=w_{11}-\alpha dw_{11} \\
w_{21}&=w_{21}-\alpha dw_{21} \\
b&=b-\alpha db \\
\end{aligned}
$$
可以看出，如果$w_{11}/w_{21}/b$都初始化为0时，由于输入$x_1/x_2$不一样，所以最后他们的变化率$dw_{11}/dw_{21}/db$都不为0，且都不相同，所以权值可以正常更新。

## 为什么神经网络所有的权值不能初始化为0

以两层的全连接网络为例，结构如下：

<img src="https://pic4.zhimg.com/80/v2-46337e9d876308a2f6041d64785795b7_720w.jpg" alt="img" style="zoom:70%;" />

前向传播公式/交叉熵损失为：
$$
\begin{aligned}
激活函数f(x)&=sigmoid(x) / ReLU(x)/... \\
无激活函数的输出z_1&=w_{11}x_1+w_{21}x_2+b_1 \\
a_{1}&=f\left(w_{11} x_{1}+w_{21} x_{2}+b_{1}\right)&=f\left(z_{1}\right) \\
a_{2}&=f\left(w_{12} x_{1}+w_{22} x_{2}+b_{2}\right)&=f\left(z_{2}\right) \\
a_{3}&=f\left(\omega_{13} a_{1}+w_{23} a_{2}+b_{3}\right)&=f\left(z_{3}\right) \\
L&=-y \log a_{3}-(1-y) \log \left(1-a_{3}\right)
\end{aligned}
$$
反向传播：
$$
\begin{aligned}
&对于最后一层激活为sigmoid而言 \\
前置条件f'(x)&=f(x)(1-f(x)) \\
即dz_1=\frac{\partial a_1}{\partial z_1}&=a_1(1-a_1) \\
--- \\
da_3=\frac{\partial L}{\partial a_3}&=-\frac{y}{a_3}+\frac{1-y}{1-a_3} \\
dw_{13}=\frac{\partial L}{\partial w_{13}}&=\frac{\partial L}{\partial a_3}\frac{\partial a_3}{\partial z_3}\frac{\partial z_3}{\partial w_{13}}=[-\frac{y}{a_3}+\frac{1-y}{1-a_3}][a_3(1-a_3)][a_1]=(a_3-y)a_1 \\
dw_{23}=\frac{\partial L}{\partial w_{23}}&=\frac{\partial L}{\partial a_3}\frac{\partial a_3}{\partial z_3}\frac{\partial z_3}{\partial w_{23}}=(a_3-y)a_2 \\
db_3=\frac{\partial L}{\partial b_3}&=\frac{\partial L}{\partial a_3}\frac{\partial a_3}{\partial z_3}\frac{\partial z_3}{\partial b_3}=(a_3-y) \\
--- \\
da_1=\frac{\partial L}{\partial a_{1}}&=\frac{\partial L}{\partial a_{3}} \frac{\partial a_{3}}{\partial z_{3}}\frac{\partial z_{3}}{\partial a_{1}}=\left(a_{3}-y\right) w_{13}\\
dw_{11}=\frac{\partial L}{\partial w_{11}}&=\frac{\partial L}{\partial a_1}\frac{\partial a_1}{\partial z_1}\frac{\partial z_1}{\partial w_{11}}=\frac{\partial L}{\partial a_1}a_1'x_1 \\
dw_{21}=\frac{\partial L}{\partial w_{21}}&=\frac{\partial L}{\partial a_1}\frac{\partial a_1}{\partial z_1}\frac{\partial z_1}{\partial w_{21}}=\frac{\partial L}{\partial a_1}a_1'x_2 \\
db_1=\frac{\partial L}{\partial b_{1}}&=\frac{\partial L}{\partial a_1}\frac{\partial a_1}{\partial z_1}\frac{\partial z_1}{\partial b_1}=\frac{\partial L}{\partial a_{1}} a_{1}^{\prime}\\
--- \\
da_2=\frac{\partial L}{\partial a_{2}}&=\frac{\partial L}{\partial a_{3}}  \frac{\partial a_{3}}{\partial z_{3}}  \frac{\partial z_{3}}{\partial a_{2}}=\left(a_{3}-y\right)w_{23}\\
dw_{12}=\frac{\partial L}{\partial w_{12}}&=\frac{\partial L}{\partial a_2}\frac{\partial a_2}{\partial z_2}\frac{\partial z_2}{\partial w_{12}}=\frac{\partial L}{\partial a_2}a_2'x_1 \\
dw_{22}=\frac{\partial L}{\partial w_{22}}&=\frac{\partial L}{\partial a_2}\frac{\partial a_2}{\partial z_2}\frac{\partial z_2}{\partial w_{22}}=\frac{\partial L}{\partial a_2}a_2'x_2 \\
db_1=\frac{\partial L}{\partial b_{1}}&=\frac{\partial L}{\partial a_2}\frac{\partial a_2}{\partial z_2}\frac{\partial z_2}{\partial b_2}=\frac{\partial L}{\partial a_{2}} \cdot a_{2}^{\prime} \\
\end{aligned}
$$

###### 当权重w和偏置b都为0时

以上各状态变化如下表
初始条件：$w_{11}=w_{21}=w_{12}=w_{22}=w_{13}=w_{23}=0,b_1=b_2=b_3=0$
下表第一行为条件，各列后续行为上一行推导出的结果

| 第一个batch<br />（第一次更新） | $a_1=a_2=f(0)\neq0,a_3=f(0)\neq0$            | 由于$w_{13}=w_{23}=0$，所以$da_1=da_2=0,a_1'=a_2'\neq0$      |
| ------------------------------- | -------------------------------------------- | ------------------------------------------------------------ |
|                                 | $dw_{13}=dw_{23}\neq0,db_3\neq0$             | $dw_{11}=dw_{21}=dw_{12}=dw_{22}=0,db_1=db_2=0$              |
|                                 | $w_{13}=w_{23}\neq0,b_3\neq0$                | $w_{11}=w_{21}=w_{12}=w_{22}=0,b_1=b_2=0$                    |
|                                 | 最后一层更新，但所有参数更新幅度一致         | 前面一层没有任何更新                                         |
| 第二个batch                     | $a_1=a_2=f(0)\neq0,a_3=f(z_3)\neq0$          | 由于$w_{13}=w_{23}\neq0$，所以$da_1=da_2\neq0,a_1'=a_2'\neq0$ |
|                                 | $dw_{13}=dw_{23}\neq0,db_3\neq0$             | 由于$x_1\neq x_2$，所以$dw_{11}=dw_{12}\neq0,dw_{21}=dw_{22}\neq0,db_1=db_2\neq0$ |
|                                 | $w_{13}=w_{23}\neq0,b_3\neq0$                | $w_{11}=w_{12}\neq0,w_{21}=w_{22}\neq0,b_1=b_2\neq0$         |
|                                 | 最后一层更新，但所有参数更新幅度一致         | 前面一层更新，但成对参数更新幅度一致，导致同一层神经元输出相同 |
| 第三个batch                     | $a_1=a_2=f(z_1)=f(z_2)\neq0,a_3=f(z_3)\neq0$ | 由于$w_{13}=w_{23}\neq0$，所以$da_1=da_2\neq0,a_1'=a_2'\neq0$ |
|                                 | $dw_{13}=dw_{23}\neq0,db_3\neq0$             | 由于$x_1\neq x_2$，所以$dw_{11}=dw_{12}\neq0,dw_{21}=dw_{22}\neq0,db_1=db_2\neq0$ |
|                                 | $w_{13}=w_{23}\neq0,b_3\neq0$                | $w_{11}=w_{12}\neq0,w_{21}=w_{22}\neq0,b_1=b_2\neq0$         |
|                                 | 最后一层更新，但所有参数更新幅度一致         | 前面一层更新，但成对参数更新幅度一致，导致同一层神经元输出相同 |
| ...                             | 往后所有batch更新都同第三个batch             |                                                              |

可以看出：

1. 第一个batch，只更新了最后一层，且最后一层更新后的权值相等，出现了权值的对称性。其余层根本不更新。
2. 第二个batch，最后一层更新结果仍然同第一个batch，权值对称。倒数第二层开始更新，但是也有**权值对称性，导致同一层神经元输出完全相同**。
3. 第三个batch，更新结构与第二个batch一样。依次类推，该网络虽然可以更新，但始终保持权值对称性，一层网络的输出永远相同，这将使网络的特征提取能力大幅降低，没有意义。
4. 这种情况不可以正常更新。

==如果中间层使用ReLU激活函数，那么在第一个epoch，$dw_{13}=dw_{23}=0,db_3\neq0$，只有偏置得到更新，往后推演可以看出，这种方法，将会导致除$db_3$之外的所有参数都无法更新，也就是ReLU导致的神经元死亡。==

###### 当权重w为0，偏置b随机初始化时

以上各状态变化如下表
初始条件：$w_{11}=w_{21}=w_{12}=w_{22}=w_{13}=w_{23}=0,b_1\neq b_2\neq b_3\neq0$
下表第一行为条件，各列后续行为上一行推导出的结果

| 第一个batch<br />（第一次更新） | $a_1\neq a_2\neq 0,a_3=f(z_3)\neq0$  | 由于$w_{13}=w_{23}=0$，所以$da_1=da_2=0,a_1'\neq a_2'\neq0$  |
| ------------------------------- | ------------------------------------ | ------------------------------------------------------------ |
|                                 | $dw_{13}\neq dw_{23}\neq0,db_3\neq0$ | $dw_{11}=dw_{21}=dw_{12}=dw_{22}=0,db_1=db_2=0$              |
|                                 | $w_{13}\neq w_{23}\neq0,b_3\neq0$    | $w_{11}=w_{21}=w_{12}=w_{22}=0,b_1=b_2=0$                    |
|                                 | 最后一层正常更新                     | 前面一层没有任何更新                                         |
| 第二个batch                     | $a_1\neq a_2\neq 0,a_3=f(z_3)\neq0$  | 由于$w_{13}\neq w_{23}\neq0$，所以$da_1\neq da_2\neq0,a_1'\neq a_2'\neq0$ |
|                                 | $dw_{13}\neq dw_{23}\neq0,db_3\neq0$ | 由于$x_1\neq x_2$，所以$dw_{11}\neq dw_{12}\neq dw_{21}\neq  dw_{22}\neq db_1\neq db_2\neq0$ |
|                                 | $w_{13}\neq w_{23}\neq0,b_3\neq0$    | $w_{11}\neq w_{12}\neq w_{21}\neq w_{22}\neq b_1\neq b_2\neq0$ |
|                                 | 最后一层正常更新                     | 前面一层正常更新                                             |
| ...                             | 往后所有batch都可以正常更新          |                                                              |

可以看出：

1. 第一个batch，最后一层正常更新。其余层不更新。
2. 第二个batch，倒数第二层也开始正常更新。依次类推，以后所有batch都可以正常更新网络。
3. 这种情况可以正常更新，但是这种方式会出现更新较慢、梯度消失、梯度爆炸的问题，一般不使用。（为什么会容易出现梯度爆炸，暂不清楚）

中间层使用ReLU激活函数，更新过程同上

###### 权重w随机初始化，偏置b为0

1. 由于$a_1\neq a_2\neq 0,a_3=f(z_3)\neq0$，所以最后一层可以正常更新。
2. 由于$w_{13}\neq w_{23}\neq 0$，所以$da_1\neq da_2\neq0,a_1'\neq a_2'\neq0$，所以前面一层也可以正常更新。
3. 这种情况可以正常更新，一般使用这种更新方法。

中间层使用ReLU激活函数，更新过程同上。

###### 相关结论

1. 上述三种情况都对应了各自的更新规律，记住以上三种情况的更新过程。不可以将所有参数初始化为0。
2. 由第一种情况可以知道，**网络中不可以存在连续相邻两层及以上的权值全部置为0**，否则就会出现权值对称性问题。
3. 当中间层为ReLU时，且所有权值都初始化为0，将导致所有参数都无法更新，如果是其他激活函数，则会出现权值对称性，导致一层神经元输出都一样相当于一个神经元。

## 为什么神经网络所有权值不能初始化为同一的常数

参考不能初始为0的情况，可以得到在第一个batch时，最后一层可以更新但权值对称，倒数第二层也可以更新但也是权值对称，因此同初始化为0的情况，**同样存在权值对称的问题**。唯一的不同就是这里第一次就可以更新所有层，而初始化为0必须第二个batch才会更新倒数第二层。

## 为什么神经网络的最后一层可以将所有权值初始化为0

1. 因为最后一层网络的输入不相同（即$a_1\neq a_2\neq0$），所以最后一层的权值依然可以正常更新。（最后一层的偏置，永远都可以正常更新，无论是否全部初始化为0）
2. 而其他层正常初始化，依然可以正常更新，所以整个网络都可以正常更新。

使用场景：

1. 如注意力机制中，一般只是为了学习特征的增量用于shortcut，理论上这个增量不能太大，因此，将注意力模块的最后一层卷积设置为0，使其在最优值附近，方便收敛，