## 类别不平衡处理方法

前提假设：小样本数量为N。

### 过采样小样本(SMOTE)，欠采样大样本

过采样：直接复制小样本，形成数量上的均衡——但会由于数据单一造成过拟合。

欠采样：随机去掉一部分多数样本，形成数量上的均衡——可能丢失一些重要的信息。
			将多数样本分成N个簇，取每个簇的中心点作为多数类的样本，再结合少数类的所有样本进行训练，这样可以保证多数类样本在特征空间的分布特性。

### 集成学习

将多数类样本随机分成多组，每组N左右个样本，然后将每组数据和所有少量类数据一起训练，这样就得到了多个分类器。最后使用bagging或者boosting等集成学习方法进行模型集成。

即：保证每个分类器样本基本平衡；也充分使用了所有数据；但时间消耗比较大。

### Focal loss

解决正负样本及难易样本不平衡。

对不同的类对应的损失函数，增加不一样的权值系数。

### OHEM

**OHEM将所有负样本按loss大小进行排序，然后根据正负样本1：3的比例去选取loss最大的负样本**。比如某个batch中共有1000个样本，正样本有50个，负样本有950个，OHEM会将这从这950个负样本挑150个loss最大的样本做为负样本，其他800个负样本的loss重置为0。这样这个batch的loss由50个正样本和150个负样本组成，维持了正负样本比例， 另外OHEM挑选的是loss最大的150个负样本，150个负样本大部分是难负样本，少部分是易负样本，解决了负样本中易负样本loss主导问题。

简单总结下OHEM的优点：

​		A. 控制了正负样本的比例为1：3

​		B. 负样本中难样本loss主导

实际使用中注意点：

​		A. 正负样本比例设置，1：3是否合适？

​		B. OHEM将大部分易负样本的loss设置为0，是否过于粗暴？

（S-OHEM）

### GHM（梯度均衡机制）

Focal Loss虽然有很好的效果，但是存在两个问题：　　

​		A.  Focal loss中的两个超参需要精细的调整，除此之外，它也是一个不会随着数据分布变化的静态loss

​		B. 如果样本中有**离群点（outliers）**，可能模型已经收敛了但是这些离群点还是会被判断错误，让模型去关注这样的样本，会影响模型的鲁棒性

GHM的想法是，**我们确实不应该过多关注易分样本，但是特别难分的样本（outliers，离群点）也不该关注！这些离群点的梯度模长d要比一般的样本大很多，如果模型被迫去关注这些样本，反而有可能 降低模型的准确度！况且，这些样本的数量也很多！那怎么同时衰减易分样本和特别难分的样本呢? 太简单了，谁的数量多衰减谁呗！那怎么衰减数量多的呢？简单啊，定义一个变量，让这个变量能衡量出一定梯度范围内的样本数量**

梯度模长g为

![img](https://img2020.cnblogs.com/blog/1483773/202006/1483773-20200602202858784-1894214848.png)

g越大，说明越难分类，g越小说明越容易分类，和focal loss中的$(1-p)^\gamma$一个含义。

因此，可以统计出所有样本的g直方图如下：（**可以看出易分样本和难分样本都比较多，因此需要对这两部分数据的权重进行衰减**）

<img src="https://img2020.cnblogs.com/blog/1483773/202006/1483773-20200602205118142-668985987.png" alt="img" style="zoom:50%;" />

GHM公式为

![img](https://img2020.cnblogs.com/blog/1483773/202006/1483773-20200602210752580-1908589.png)

**梯度密度（GD）**的含义：**单位梯度模长g部分的样本个数**，就是上图每个g对应的高度。



#### 参考链接

[样本不均衡问题](https://www.cnblogs.com/silence-cho/p/12987476.html)

## 狭长目标的处理方法

	1. 增加一个狭长的anchor，用于对狭长目标的检测。
	2. 使用DCN。
	 目标多为细长状，形状不规则，那么使用DCN可以更好的学习不规则的特征。