## 问题

梯度消失无论是笔试还是面试都是常客了，其实对应于梯度消失，还有一个梯度爆炸的概念，这又是什么导致的呢？下面我们将根据公式推导来解释何为梯度消失与梯度爆炸。

## 梯度消失和梯度爆炸的表现

网络层数越多，模型训练的时候便越容易出现 梯度消失(gradient vanish) 和 梯度爆炸(gradient explod) 这种梯度不稳定的问题。假设现在有一个含有３层隐含层的神经网络：

![这里写图片描述](https://gitee.com/xn1997/picgo/raw/master/SArqy1bp56euwIV.png)

**梯度消失发生时的表现是：**靠近输出层的 hidden layer 3 的权值更新正常，但是靠近输入层的 hidden layer 1 的权值更新非常慢，导致其权值几乎不变，仍接近于初始化的权值。这就导致 hidden layer 1 相当于只是一个映射层，对所有的输入做了一个函数映射，这时的深度学习网络的学习等价于只有后几层的隐含层网络在学习。

**梯度爆炸发生时的表现是：**当初始的权值太大，靠近输入层的 hidden layer 1 的权值变化比靠近输出层的 hidden layer 3 的权值变化更快。

所以梯度消失和梯度爆炸都是出现在靠近输入层的参数中。

## 产生梯度消失与梯度爆炸的根本原因

模型的梯度更新是基于反向传播过程。即根据损失函数计算出的误差通过<u>梯度反向传播</u>的方式计算出各层权值的梯度变化，从而进行权值的更新。而这个反向传播的过程使用的是<u>链式求导</u>的规则。网络越深，连乘的项就越多，如果这些<u>连乘项大于或者小于1</u>，就很容易造成最终的梯度值特别大，或者接近0。

梯度爆炸的原因：权值初始化太大。

梯度消失的原因：使用了不合适的激活函数（sigmoid）

### 梯度消失分析

下图是我画的一个非常简单的神经网络，每层都只有一个神经元，且神经元所用的激活函数 $\sigma$ 为 sigmoid 函数，$Loss$ 表示损失函数，前一层的输出与后一层的输入关系如下：
$$
y_i = \sigma(z_i) = \sigma(w_i*x_i+b_i), \quad其中x_i = y_{i-1}
$$
![](https://gitee.com/xn1997/picgo/raw/master/aplXMCsvPw4rh2Q.jpg)

因此，根据反向传播的链式法则，损失函数相对于参数 $b_1$ 的梯度计算公式如下：
$$
\frac{\partial Loss}{\partial b_1} = \frac{\partial Loss}{\partial y_4}*\frac{\partial y_4}{\partial z_4}*\frac{\partial z_4}{\partial x_4}*\frac{\partial x_4}{\partial z_3}*\frac{\partial z_3}{\partial x_3}*\frac{\partial x_3}{\partial z_2}*\frac{\partial z_2}{\partial x_2}*\frac{\partial x_2}{\partial z_1}*\frac{\partial z_1}{\partial b_1} \\
= \frac{\partial Loss}{\partial y_4}*\partial{'}(z_4)*w_4*\partial{'}(z_3)*w_3*\partial{'}(z_2)*w_2*\partial{'}(z_1)
$$
而 sigmoid 函数的导数 $\sigma{'}(x)$ 如下图所示：

![这里写图片描述](https://gitee.com/xn1997/picgo/raw/master/eca5HdVqL9EBmuU.png)

即 $\sigma{'}(x)\le \frac{1}{4}$ ，而我们一般会使用标准方法来初始化网络权重，即使用一个均值为 0 标准差为 1 的高斯分布，因此初始化的网络参数 $w_i$ 通常都小于 1 ，从而有 $|\sigma{'}(z_i)*w_i|\le \frac{1}{4}$ 。根据公式(2)的计算规律，层数越多，越是前面的层的参数的求导结果越小，于是便导致了梯度消失情况的出现。

### 梯度爆炸分析

在分析梯度消失时，我们明白了导致其发生的主要原因是　$|\sigma{'}(z_i)*w_i|\le \frac{1}{4}$ ，经链式法则反向传播后，越靠近输入层的参数的梯度越小。而导致梯度爆炸的原因是：$|\sigma{'}(z_i)*w_i|>1$，当该表达式大于 1 时，经链式法则的指数倍传播后，前面层的参数的梯度会非常大，从而出现梯度爆炸。

但是要使得$|\sigma{'}(z_i)*w_i|>1$，就得 $|w_i| > 4$才行，按照 $|\sigma{'}(w_i*x_i+b_i)*w_i|>1$，可以计算出 $x_i$ 的数值变化范围很窄，仅在公式(3)的范围内，才会出现梯度爆炸，**因此梯度爆炸问题在使用 sigmoid 激活函数时出现的情况较少，不容易发生。**

![这里写图片描述](https://gitee.com/xn1997/picgo/raw/master/dIgKDuw976leTF4.png)

1. 

## 梯度爆炸带来的现象

1. **模型训练不稳定，loss会突然变大或者变小。**
2. **极端情况下，权重的值变的很大，以至于溢出，造成损失为NaN。**

## 怎么解决

如上分析，**造成梯度消失和梯度爆炸问题是网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应**。另外一个原因是当激活函数使用 sigmoid 时，梯度消失问题更容易发生，因此可以考虑的解决方法如下：

1. 压缩模型层数
2. 改用其他的激活函数如 ReLU
3. 使用 BN 层
4. 使用 ResNet 的短路连接结构

### 梯度爆炸的解决方法

1. **pre-training+fine-tunning**

   无监督逐层训练方法，其基本思想是<u>每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入</u>，此过程就是逐层“预训练”（pre-training），保证每层的输出都一样（和BN保持输入输出方差一致思想一样）；在预训练完成后，再对整个网络进行“微调”（fine-tunning）。此思想相当于是先寻找局部最优，然后整合起来寻找全局最优，此方法有一定的好处，但是目前应用的不是很多了。

2. **梯度剪切：对梯度设定阈值**

   梯度剪切这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。

3. **权重正则化**

   另外一种解决梯度爆炸的手段是采用权重正则化（weithts regularization），正则化主要是通过对网络权重做正则来限制过拟合。如果发生梯度爆炸，那么权值就会变的非常大，反过来，通过正则化项来限制权重的大小，也可以在一定程度上防止梯度爆炸的发生。比较常见的是 L1 正则和 L2 正则，在各个深度框架中都有相应的API可以使用正则化。

4. **改用其他的激活函数如 ReLU**

5. **batch normalization**

   BN就是通过对每一层的输出规范为均值和方差一致的方法，消除了权重参数放大缩小带来的影响，进而解决梯度消失和爆炸的问题，或者可以理解为BN将输出从饱和区拉倒了非饱和区。

### 梯度消失的解决方法

1. **改用其他的激活函数如 ReLU**

2.  **batch normalization**

   BN就是通过对每一层的输出规范为均值和方差一致的方法，消除了权重参数放大缩小带来的影响，进而解决梯度消失和爆炸的问题，或者可以理解为BN将输出从饱和区拉倒了非饱和区。

3. **使用 ResNet 的短路连接结构**

## 参考资料

[激活函数及其作用以及梯度消失、爆炸、神经元节点死亡的解释](https://blog.csdn.net/qq_17130909/article/details/80582226)