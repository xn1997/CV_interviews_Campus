理论假设：

输入和输出的方差保存一致，这样就可以避免在信息传导过程中，出现梯度爆炸或者梯度消失的情况，利于训练。

条件：**<u>正向传播时，激活值的方差保持不变；反向传播时，关于状态值的梯度的方差保持不变。</u>**



参数初始化的理想状态是参数正负各半，期望为0。

**过大或者过小的初始化**

如果权值的初始值过大，则会导致**梯度爆炸**，使得网络不收敛；过小的权值初始值，则会导致梯度消失，会导致网络**收敛缓慢或者收敛到局部极小值**。

如果权值的初始值过大，则loss function相对于权值参数的梯度值很大，每次利用梯度下降更新参数的时，参数更新的幅度也会很大，这就导致loss function的值在其最小值附近震荡。

而过小的初值值则相反，loss关于权值参数的梯度很小，每次更新参数时，更新的幅度也很小，着就会导致loss的收敛很缓慢，或者在收敛到最小值前在某个局部的极小值收敛了。

<img src="https://pic4.zhimg.com/v2-19a4b4a9ffd5831e54e8e47192f6f292_1440w.jpg?source=172ae18b" alt="权重/参数初始化" style="zoom:80%;" />

## 随机初始化

过小导致梯度消失，过大导致梯度爆炸。

## 随机初始化带有BN

BN可以将过大过小的数据都归一化到0-1分布，避免了梯度爆炸或梯度消失的情况。

## Xavier 初始化

泽维尔初始化

基本思想：**保证输入和输出的数据分布一致，即保证输入输出的均值和方差一致**（数据分布等同于均值和方差），使得网络有更好的信息流动。数据分布一致，那么网络也就更加容易学习（这点和BN的作用一样）。
为了使得网络中信息更好的流动，**每一层输出的方差应该尽量相等。**
基于这个目标，去推导一下：每一层的权重应该满足哪种条件。



**Xavier初始化主要用于tanh，不适用于ReLU。**

**根据输入和输出神经元的数量自动决定初始化的范围：定义参数所在的层的输入维度为 ![[公式]](https://www.zhihu.com/equation?tex=m) ，输出维度为 ![[公式]](https://www.zhihu.com/equation?tex=n) ，那么参数将从 ![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%28+-%5Csqrt%7B%5Cfrac%7B6%7D%7Bm%2Bn%7D%7D%2C%5Csqrt%7B%5Cfrac%7B6%7D%7Bm%2Bn%7D%7D+%5Cright%29) 均匀分布中采样。**



**为什么方差相等，就可以使得网络有更好的信息流动？**

本文的思想在考虑线性激活函数的情况下, 在初始化的时候使各层神经元的方差保持不变, 即使各层有着相同的分布. 如果每层都用N(0, 0.01)随机初始化的话, 各层的数据分布不一致, 随着层度的增加, 神经元将集中在很大的值或很小的值, 不利于传递信息. 很多初始化策略都是为了保持每层的分布不变, 而BN是通过增加归一化层使得每层数据分布保持在N(0, 1)。

xavier的初始化方式和BN一样，为了保证数据的分布（均值方差一致）是一样的，加快收敛，就这么简单吧。

### 参考链接

[深度学习——Xavier初始化方法](https://blog.csdn.net/shuzfan/article/details/51338178)——推导的很好，很明了，可以看懂上述分布如何来的。

## He初始化

RELU中非常适合：

Xavier初始化适合关于0点对称的激活函数，不适合Relu。

主要想要解决的问题是由于经过relu后，方差会发生变化，因此我们初始化权值的方法也应该变化。只考虑输入个数时，MSRA初始化是一个均值为0，方差为2/n的高斯分布：

![img](https://pic2.zhimg.com/80/v2-ffea82cf87c2b8768fd3e051692828bd_720w.jpg)

**He initialization的思想是：**在ReLU网络中，假定每一层有一半的神经元被激活，另一半为0，所以要保持variance不变，只需要在Xavier的基础上再除以2：

```text
w = np.random.randn(node_in, node_out) / np.sqrt(node_in/2)
```

## 参考链接

[权重/参数初始化](https://zhuanlan.zhihu.com/p/72374385#:~:text=%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96%20%E5%8F%88%E7%A7%B0%E4%B8%BA%20%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96%20%EF%BC%88weight%20initialization%EF%BC%89%E6%88%96%20%E6%9D%83%E5%80%BC%E5%88%9D%E5%A7%8B%E5%8C%96,%E3%80%82%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E7%9A%84%E6%9C%AC%E8%B4%A8%E6%98%AF%E5%AF%B9weight%EF%BC%88%E5%8D%B3%E5%8F%82%E6%95%B0%20W%EF%BC%89%E8%BF%9B%E8%A1%8C%E6%9B%B4%E6%96%B0%EF%BC%8C%E8%BF%99%E9%9C%80%E8%A6%81%E6%AF%8F%E4%B8%AA%E5%8F%82%E6%95%B0%E6%9C%89%E7%9B%B8%E5%BA%94%E7%9A%84%E5%88%9D%E5%A7%8B%E5%80%BC%E3%80%82%20%E8%AF%B4%E7%99%BD%E4%BA%86%EF%BC%8C%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%B6%E5%AE%9E%E5%B0%B1%E6%98%AF%E5%AF%B9%E6%9D%83%E9%87%8D%E5%8F%82%E6%95%B0w%E4%B8%8D%E5%81%9C%E5%9C%B0%E8%BF%AD%E4%BB%A3%E6%9B%B4%E6%96%B0%EF%BC%8C%E4%BB%A5%E8%BE%BE%E5%88%B0%E8%BE%83%E5%A5%BD%E7%9A%84%E6%80%A7%E8%83%BD%E3%80%82%20%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96%E5%AF%B9%E4%BA%8E%E7%BD%91%E7%BB%9C%E7%9A%84%E8%AE%AD%E7%BB%83%E5%BE%88%E9%87%8D%E8%A6%81%EF%BC%8C%E4%B8%8D%E5%A5%BD%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8F%82%E6%95%B0%E4%BC%9A%E5%AF%BC%E8%87%B4%E6%A2%AF%E5%BA%A6%E4%BC%A0%E6%92%AD%E9%97%AE%E9%A2%98%EF%BC%8C%E9%99%8D%E4%BD%8E%E8%AE%AD%E7%BB%83%E9%80%9F%E5%BA%A6%EF%BC%9B%E8%80%8C%E5%A5%BD%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8F%82%E6%95%B0%E8%83%BD%E5%A4%9F%E5%8A%A0%E9%80%9F%E6%94%B6%E6%95%9B%EF%BC%8C%E5%B9%B6%E4%B8%94%E6%9B%B4%E5%8F%AF%E8%83%BD%E6%89%BE%E5%88%B0%E8%BE%83%E4%BC%98%E8%A7%A3%E3%80%82%20%E5%A6%82%E6%9E%9C%E6%9D%83%E9%87%8D%E4%B8%80%E5%BC%80%E5%A7%8B%E5%BE%88%E5%B0%8F%EF%BC%8C%E4%BF%A1%E5%8F%B7%E5%88%B0%E8%BE%BE%E6%9C%80%E5%90%8E%E4%B9%9F%E4%BC%9A%E5%BE%88%E5%B0%8F%EF%BC%9B%E5%A6%82%E6%9E%9C%E6%9D%83%E9%87%8D%E4%B8%80%E5%BC%80%E5%A7%8B%E5%BE%88%E5%A4%A7%EF%BC%8C%E4%BF%A1%E5%8F%B7%E5%88%B0%E8%BE%BE%E6%9C%80%E5%90%8E%E4%B9%9F%E4%BC%9A%E5%BE%88%E5%A4%A7%E3%80%82)

[关于参数初始化的若干问题以及Xavier、He初始化推导](https://zhuanlan.zhihu.com/p/40175178)——Xavier看这个推导就可以了（He貌似有点小问题）
