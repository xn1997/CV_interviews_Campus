## FairMOT

复赛期间对于多目标跟踪任务使用的baseline。

本质属于**联合学习检测和嵌入模型（Joint Detection and Embedding，JDE）**

### 改进点

#### 网络结构

1. 使用anchor free替换anchor base。
   1. anchor base可能导致多个anchor对应一个目标，对于reid分支会造成歧义
2. 多特征融合
   1. 类似FPN，将高层信息不断的上采样和底层信息融合，文中称之为**深层聚合算子（Deep Layer Aggregation，DLA）**，改进是在ResNet基础上做的
3. 添加一个和检测头平行的reid分支，提取每个目标的reid特征（文中是128，但程序里是512）
   1. 实现检测和reid的联合训练

#### loss

1. 检测分支
   1. 同普通的anchor free损失
   2. heatmap loss：计算所有像素点的损失
   3. +focal loss：在heatmap计算出每个位置存在目标的置信度后，添加一个focal loss
   4. offset（中心偏移）和size（框大小）loss：直接使用L1 loss（绝对值损失）
2. reid分支
   1. 使用CE loss：将每个ID视为一类，来训练reid特征向量

# 参考链接

[多目标跟踪 | FairMOT：统一检测、重识别的多目标跟踪框架，全新Baseline](https://cloud.tencent.com/developer/article/1634149)

## PANDA比赛

### round1

#### 训练

1. 图片切割为2560*2560
2. 只保留IOU大于0.5的目标
3. 如果添加0.5和0.1等多尺度训练效果会更好

**原因：**

尽可能的保证图像中的目标在切割时不要被切断，因此将原图缩放后在切割是很有必要的，不然原图下近处的目标会被切断，无法参与训练。

#### 前后处理

1. 图片缩放到为1，0.5，0.1三个尺度，然后按照overlap=0.5的步长统一切割为2560*2560
2. 对于每个patch的预测结果，如果其距离patch边界小于10个像素就直接剔除
3. 综合所有patch的预测结果，并进行nms
   1. 置信度阈值由0.5变为0，提高4个点
   2. NMS的IOU换为DIOU，提高1个点

**原因：**

1. 多尺度可以保证，即可以检测到远处的小目标，也可以检测到近处的大目标（因为0.1尺度下，近处的目标就不会被分割开了）
2. 剔除边缘的目标：靠近边缘的预测结果，可以视为当前目标只有部分在该patch内，那么显然根据部分信息得到的预测框必然精度不高，而且overlap=0.5，该目标必然会在另一个overlap里完全出现，不必担心漏检该目标
3. 置信度为0：经过实验，阈值设置高了，会导致大量漏检，因此设为0可以减少漏检，同时结合2减少误检，效果有所提升

#### 网络结构

1. cascade rcnn
   1. 检测效果好
   2. 在mmdetection框架下，相比于FCOS等anchor free的模型，在开启FP16时，训练稳定且可以将batch提高，减少内存，而其他模型FP16会导致loss为nan（初步猜测是因为梯度消失），或者内存减少不明显，因此该模型对于显存较小的机器很友好
2. DCN
   1. 在stage2,3,4的最后添加DCN模块
   2. 可以提取出适应目标形状的特征，相较于传统卷积，提取的特征更能覆盖目标

### round2

#### 训练

1. 同round1，在原尺度切片
2. 同round1，标签增加了ID类，用于跟踪

#### 前后处理

与round1完全相同

#### 网络结构

1. FairMOT

**原因：**

1. 使用anchor free作为检测模型，可以更好的和reid结合
2. 将检测和reid放在一个网络，相比D&T两阶段跟踪结构，速度更快