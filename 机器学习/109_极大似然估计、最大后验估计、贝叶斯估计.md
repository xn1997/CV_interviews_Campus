最大似然估计（MLE）：经验风险最小化

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Chat%7B%5Ctheta%7D_%5Ctext%7BMLE%7D+%26%3D+%5Carg+%5Cmax+P%28X%3B+%5Ctheta%29+%5C%5C+%26%3D+%5Carg+%5Cmax+P%28x_1%3B+%5Ctheta%29+P%28x_2%3B+%5Ctheta%29+%5Ccdot%5Ccdot%5Ccdot%5Ccdot+P%28x_n%3B%5Ctheta%29+%5C%5C+%26+%3D+%5Carg+%5Cmax%5Clog+%5Cprod_%7Bi%3D1%7D%5E%7Bn%7D+P%28x_i%3B+%5Ctheta%29+%5C%5C+%26%3D+%5Carg+%5Cmax+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%5Clog+P%28x_i%3B+%5Ctheta%29+%5C%5C+%26%3D+%5Carg+%5Cmin+-+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%5Clog+P%28x_i%3B+%5Ctheta%29+%5Cend%7Balign%2A%7D)

最大后验估计（MAP）：结构风险最小化（引入了先验概率$P(\theta)$）

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Chat%7B%5Ctheta%7D_%5Ctext%7BMAP%7D+%26%3D+%5Carg+%5Cmax+P%28%5Ctheta+%7C+X%29+%5C%5C+%26%3D+%5Carg+%5Cmin+-%5Clog+P%28%5Ctheta+%7C+X%29+%5C%5C+%26+%3D+%5Carg+%5Cmin+-%5Clog+P%28X%7C%5Ctheta%29+-+%5Clog+P%28%5Ctheta%29+%2B+%5Clog+P%28X%29+%5C%5C+%26%3D+%5Carg+%5Cmin+-%5Clog+P%28X%7C%5Ctheta+%29+-+%5Clog+P%28%5Ctheta%29+%5Cend%7Balign%2A%7D)

这里$\text{log}P(X|\theta)=\sum^{n}_{i=1}\text{log}P(x_i;\theta)$

所以MAP和MLE在优化时的不同就是在于增加了一个先验项![[公式]](https://www.zhihu.com/equation?tex=-+%5Clog+P%28%5Ctheta%29)。

通过以上的分析可以大致给出他们之间的联系： ![[公式]](https://www.zhihu.com/equation?tex=MAP%28%5Ctheta%29%5Capprox+MLE%28%5Ctheta%29%2BP%28%5Ctheta%29) 。



经验风险最小化：只根据数据估计出参数

结构风险最小化：除了根据数据估计之外，还引入了先验知识，因此在数据少时MAP更合理，而数据多时MAP会趋向于MLE。

二者都是利用实现估计好的预测模型来估计的，比如假设数据服从高斯分布。

### 参数链接

[聊一聊机器学习的MLE和MAP：最大似然估计和最大后验估计](https://zhuanlan.zhihu.com/p/32480810)

### 一些问题

3.下列关于极大似然估计（Maximum Likelihood Estimate，MLE），说法正确的是？

A. MLE 可能并不存在

B. MLE 总是存在

C. 如果 MLE 存在，那么它的解可能不是唯一的

D. 如果 MLE 存在，那么它的解一定是唯一的

答案：AC

解析**：如果极大似然函数 L(θ) 在极大值处不连续，一阶导数不存在，则 MLE 不存在；另一种情况是 MLE 并不唯一，极大值对应两个θ。**

## 朴素贝叶斯算法

![img](https://pic1.zhimg.com/80/v2-15b16ce6d37b616a5443c0f7e42e03ec_720w.png)

就是利用这个公式来计算的，很简单，根本不需要训练，每次给定A，去数据集中计算出后面的P(A|B)即可，不要怀疑就是最简单的那种。

参考链接：[带你理解朴素贝叶斯分类算法](https://zhuanlan.zhihu.com/p/26262151)

