## 问题

**逻辑回归**相比于**线性回归**，有何异同

## 问题背景

逻辑回归与线性回归是机器学习中非常常用和基础的模型，面试当中也会经常被问到，而两者有着很多共同点，就连名字都差不多一样，所以会被问道两者的区别也是太正常不过了。我们都知道，回归是对连续值的预测，那么“逻辑回归”是回归问题吗？

我们首先聊聊**逻辑回归这个名字的由来**：对逻辑回归公式进行整理，我们可以得到 $log{\frac{p}{1-p}} = \omega^T x+b$，其中$p = P(y=1|x)$ 也就是将给定输入`x`预测为正类的概率。如果将一个事件的几率(odd)定义为该事件发生的概率与不发生的概率的比值$\frac{p}{1-p}$，那么逻辑回归就可以看做是对 $y = 1 | x$ 这一事件的**对数几率**的线性回归，于是“逻辑回归”这个称谓就延续了下来。

## 问题解答

**两者最本质的区别**：逻辑回归处理的是分类问题，而线性回归处理的是回归问题。

**两者最大的区别**：逻辑回归将`y`视为因变量而不是$\frac{p}{1-p}$，而`y`是类别标签，如二分类中`y`可以是 0 或 1 ，是一个离散值，但线性回归中的因变量是连续值。

**两者相同之处**：

1. 都是使用**极大似然估计法来估计模型参数**：这里可能会有点疑问是说线性回归中明明是使用均方误差即最小二乘法来估计的，不是极大似然估计呀。其实实际上在预测值与真实值之间的误差满足正态分布的假设下，最小化均方误差与极大似然估计在本质上是一样的。
2. 二者在求解超参数的过程中，**都可以使用梯度下降的方法**，这也是监督学习中一个常见的相似之处

<u>以上内容参考自《百面机器学习》的逻辑回归一节</u>



## 拓展

这里的拓展主要是线性回归与逻辑回归的模型定义与求解过程。

### 线性回归

线性回归试图学得
$$
f(x_i) = \omega x_i + b，使得f(x_i)\approx y_i \\　其中y_i为真实值，而f(x_i)为预测值
$$
那怎么去确定参数 $\omega$ 和 $b$ 呢，显然关键在于怎么去衡量 $f(x_i)$ 与 $y_i$ 之间的距离，而在回归任务中一般使用均方误差，因此我们的目标变为让均方误差最小化：
$$
(\omega^*, b) = arg min_{(\omega, b)}\sum_{i=1}^{m}({f(x_i)}-y_i)^2
$$
接下来的模型参数求解可以对 $\omega$ 与 $b$ 进行求导或者使用梯度下降法。

### 逻辑回归

在线性回归的基础上，逻辑回归可以理解为只需找到一个单调可微函数将线性模型的预测值与把该输入归类为正类的概率$ p = P(y = 1 | x)$ 联系起来即可，即将线性回归得到的连续值映射为二分类的类别0或1，因此我们自然而然就想到了sigomid函数，它的函数形式如下:
$$
　　　y = \frac{1}{1+e^{-z}}　\\ 我们令z = \omega^Tx + b，令y为把该输入x归类为为正类的概率p，便可以得到：\\ p = \frac{1}{1+e^{-(\omega^T+b)}}
$$
对以上公式稍做变换就可以得到
$$
p = P(y=1|x)= \frac{e^{\omega^Tx+b}}{1+e^{\omega^Tx+b}} \\ 1-p = P(y=0|x)= \frac{1}{1+e^{\omega^Tx+b}}  \\ log{\frac{p}{1-p}} = \omega^T x+b
$$
#### 使用极大似然估计来估计模型参数

逻辑回归模型的数学形式确定后，剩下就是如何去求解模型中的参数。在统计学中，常常使用极大似然估计法来求解，即找到一组参数，使得在这组参数下，我们的数据的似然度（概率）最大。

设： 

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+P%28Y%3D1%7Cx%29+%26%3D+p%28x%29+%5C%5C++P%28Y%3D0%7Cx%29+%26%3D+1-+p%28x%29+%5Cend%7Baligned%7D%5C%5C)

似然函数： 

![[公式]](https://www.zhihu.com/equation?tex=L%28w%29%3D%5Cprod%5Bp%28x_%7Bi%7D%29%5D%5E%7By_%7Bi%7D%7D%5B1-p%28x_%7Bi%7D%29%5D%5E%7B1-y_%7Bi%7D%7D++%5C%5C)

为了更方便求解，我们对等式两边同取对数，写成对数似然函数： 

![[公式]](https://www.zhihu.com/equation?tex=+%5Cbegin%7Baligned%7D+L%28w%29%26%3D%5Csum%5By_%7Bi%7Dlnp%28x_%7Bi%7D%29%2B%281-y_%7Bi%7D%29ln%281-p%28x_%7Bi%7D%29%29%5D+%5C%5C+%26%3D%5Csum%5By_%7Bi%7Dln%5Cfrac%7Bp%28x_%7Bi%7D%29%7D%7B1-p%28x_%7Bi%7D%29%7D%2Bln%281-p%28x_%7Bi%7D%29%29%5D++%5C%5C+%26%3D%5Csum%5By_%7Bi%7D%28w+%5Ccdot+x_%7Bi%7D%29+-+ln%281%2Be%5E%7Bw+%5Ccdot+x_%7Bi%7D%7D%29%5D+%5Cend%7Baligned%7D+%5C%5C)

#### 对于逻辑回归的理解

1. 逻辑回归要解决的是分类问题，而线性回归处理的是回归问题，为了建立其回归与分类之间的关系，因此就考虑到使用sigmoid函数。

2. 输入就是特征x，而输出就是属于该类的概率，如式（3），该式就是逻辑回归的公式。

   可以通过对（3）式进行变换得到公式（4），而该式的右侧就是线性回归，所以就沿用了“回归”一词，不过这里的回归是指对“$y=1|x$这一事件的对数几率”的回归。

### 参考链接

[【机器学习】逻辑回归（非常详细）](https://zhuanlan.zhihu.com/p/74874291)

