# 待整理

[机器学习面试SVM常见问题汇总](https://blog.csdn.net/qq_38489833/article/details/107055543?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-0.control&spm=1001.2101.3001.4242)

什么是SVM？

### 1. SVM 原理

SVM 是一种二类分类模型。是在特征空间中寻找间隔最大化的分离超平面的线性分类器。

- 当训练样本<u>线性可分时，通过硬间隔最大化</u>，学习一个线性分类器，即线性可分支持向量机；
- 当训练数据<u>近似线性可分时，引入松弛变量，通过软间隔最大化</u>，学习一个线性分类器，即线性支持向量机；
- 当训练数据<u>线性不可分时，通过使用核技巧</u>及软间隔最大化，学习非线性支持向量机。



**最小化问题**中的条件，一个是 <u>h(x)<=0</u>， 另一个是λ>=0，并且在拉格朗日式子中<u>λ前面的符号是正的</u>。如果你的<u>限制条件 h(x)>=0</u>, 那么<u>λ前面的符号需要改为负的</u>

[约束最优化问题：原问题和对偶问题，以及拉格朗日因子的符号](https://blog.csdn.net/robot_learner/article/details/104218621)

1. 间隔

   最大化间隔，得出SVM的损失函数。（带有约束，约束是保证分类结果正确）

2. 对偶

   根据拉格朗日乘子法，将约束问题变为无约束问题，再转化为对偶问题，然后求解即可。

3. 核技巧

   SVM是线性分类器，而核函数可以将对x进行非线性变换，从而使SVM+kernal实现非线性分类。

##### 1.2.对偶：

原来的问题是一个凸优化问题 ， 看到凸优化问题一般会想到， 利用拉格朗日乘子法， 将有约束的问题转换为无约束的问题， 在将无约束的原问题转化为对偶问题(注意这里面的KKT条件)， 进而利用SMO(序列最小优化)来解决这个对偶问题。

##### 1.3.核技巧

**核函数将数据映射到更高维的空间后处理**，但不用做这种显式映射，而是<u>先对两个样本向量做内积，然后用核函数映射。这等价于先进行映射，然后再做内积</u>。

核函数的思想在于它事先在低维上进行计算，而将实质上的分类效果表现在了高维上，也就是避免了直接在高维空间的复杂计算。核技巧有助于解决非线性SVM的问题。

##### 1.3.1 常见的核函数有

线性核（Linear核）
多项式核 （Polynomial核）
高斯核（RBF核 / 径向基核

##### 1.3.2 核函数的选取方法

如果<u>特征的数量大到和样本数量差不多</u>，则选用LR或者线性核的SVM；(特征大，选用泛化性更好的线性核)
如果<u>特征的数量小</u>，样本的数量正常，则选用SVM+高斯核函数；（特征小，
如果特征的数量小，而样本的数量很大，则需要手工添加一些特征从而变成第一种情况

## 点到直线的距离

![[公式]](https://www.zhihu.com/equation?tex=d%3D%5Cfrac%7B%7CAx_0%2BBy_0%2BC%7C%7D%7B%5Csqrt%7BA%5E2%2BB%5E2%7D+%7D)

这里分类超平面为$g(x)=wx+b$

## 凸函数、凸集

凸函数：一阶导递增（二阶导恒大于0）

## 参考链接

[SVM 高频面试题](https://zhuanlan.zhihu.com/p/43827793)

[【ML-QA-02】支持向量机SVM中常见的面试问题QA](https://zhuanlan.zhihu.com/p/99027375)

