# 信息熵、交叉熵、相对熵（KL散度）

参考链接：[一文彻底搞懂信息熵、相对熵、交叉熵和条件熵（含例子）](https://blog.csdn.net/qq_36931982/article/details/82998081)

- **信息熵**

$$
H(X)=-\sum_{i=1}^N{p(x_i)\log p(x_i)}
$$



一个系统越是有序，信息熵就越低；反之，一个系统越是混乱，信息熵就越高

- **交叉熵**

$$
H(X)=-\sum_{i=1}^N{p(x_i)\log q(x_i)}
$$



主要==度量两个概率分布间的差异性信息==。

交叉熵函数一般作为损失函数，用于评价预测值和样本值之间的差异。利用KL散度也可以，但是由于交叉熵加上信息熵可以得出KL散度，所以在优化过程中我们也只需要关注交叉熵就行，简化复杂度。

- **相对熵**

$$
D_{K L}(p \| q)= \sum_{i} p(i) * \log \frac{p(i)}{q(i)}=
\sum_{i=1}^{N} p\left(x_{i}\right) \cdot\left(\log p\left(x_{i}\right)-\log q\left(x_{i}\right)\right)
$$

相对熵 =  交叉熵 - 信息熵

也是==度量两个概率分布间的差异性信息==。相对熵的值越小，表示q分布和p分布越接近。

## 相对熵计算示例

比如随机变量$X∼P$取值为1,2,3时的概率分别为$[0.2,0.4,0.4]$，随机变量$Y∼P$取值为1,2,3时的概率分别为$[0.4,0.2,0.4]$，则：
$$
\begin{aligned}
D(P \| Q) &=0.2 \times \log \left(\frac{0.2}{0.4}\right)+0.4 \times \log \left(\frac{0.4}{0.2}\right)+0.4 \times \log \left(\frac{0.4}{0.4}\right) \\
&=0.2 \times-0.69+0.4 \times 0.69+0.4 \times 0 \\
&=0.138
\end{aligned}
$$
由上述计算方法可知，KL散度的性质为：

1. $D_{KL}(P||Q)≥0$，即**非负性**。
2. $D_{KL}(P||Q)\neq D_{KL}(Q||P)$，即**不对称性**。

Python代码实现，离散型KL散度可通过`SciPy`进行计算：

```python
from scipy import stats

P = [0.2, 0.4, 0.4]
Q = [0.4, 0.2, 0.4]
stats.entropy(P,Q) # 0.13862943611198905

P = [0.2, 0.4, 0.4]
Q = [0.5, 0.1, 0.4]
stats.entropy(P,Q) # 0.3195159298250885

P = [0.2, 0.4, 0.4]
Q = [0.3, 0.3, 0.4]
stats.entropy(P,Q) # 0.03533491069691495
```

