### 随机森林如何处理缺失值

1. 对于缺失的数值型变量，用其对应类别的中位数代替。
   对于缺失的描述性变量，用其对应类别中出现次数最多的数值代替。
2. 同1也是**使用中位数和出现最多的数替换**，但引入了权重。
   先计算要替换的数据和其他同类数据**计算相似度**，根据相似度确定缺失值是否更靠近该数据点。

- 首先，给缺失值预设一些估计值，比如数值型特征，**选择其余数据的中位数或众数作为当前的估计值**
- 然后，根据估计的数值，建立随机森林，把所有的数据放进随机森林里面跑一遍。**记录每一组数据在决策树中一步一步分类的路径**.
- **判断哪组数据和缺失数据路径最相似**，引入一个相似度矩阵，来记录数据之间的相似度，比如有N组数据，相似度矩阵大小就是N*N
- 如果缺失值是类别变量，**通过权重投票得到新估计值**，如果是数值型变量，**通过加权平均得到新的估计值**，如此迭代，**直到得到稳定的估计值**。

**解释相似度矩阵**：
相似度矩阵就是任意两个观测实例间的相似度矩阵，原理是<u>如果两个观测实例落在同一棵树的相同节点次数越多，则这两个观测实例的相似度越高</u>。

详细来说：
Proximity 用来衡量两个样本之间的相似性。原理就是如果两个样本落在树的同一个叶子节点的次数越多，则这两个样本的相似度越高。当一棵树生成后，让数据集通过这棵树，<u>落在同一个叶子节点的”样本对(xi,xj)” proximity 值 P(i,j) 加 1</u> 。所有的树生成之后，利用树的数量来归一化 proximity matrix。继而，我们得到缺失值所在样本的权重值，权重值相近的可以用于缺失值的填补参考。

#### 参考链接

[随机森林如何处理缺失值](https://www.jianshu.com/p/a4bf9224d66c)

### 决策树中的解决方案（《机器学习》中的介绍）

**这就是C4.5的缺失值处理方法**

1. 直接剔除含有缺失值的样本。
   一般缺失样本很多，剔除后只有少量样本，不合理。

#### 问题1：带有缺失值的属性如何计算信息增益（进行属性划分）？

![image-20210818235352490](https://raw.githubusercontent.com/xn1997/picgo/master/image-20210818235352490.png)
$$
\begin{aligned}
Gain(D,a):&数据集D以a属性划分的信息增益\\
\tilde{D}:&无缺失值的样本集\\
\text{Ent}(D):&数据集D的信息熵\\
不存在缺失值的&信息增益公式（用于和上面的进行对比）\\
Gain(D,a) =& Ent(D) - \sum_{v=1}^{V}\frac{|D^v|}{|D|}Ent(D^v)
\end{aligned}
$$

###### 总结

**带有缺失值的信息增益就是增加了一个权值，该权值就是无缺失值样本的比例**，其余的就是普通的信息增益计算方法。

#### 问题2：如果样本在该属性的值缺失，该如何划分该样本？

将该样本划入所有的子节点，只不过增加一个样本权值，样本权值就是**各个子节点无缺失样本数目占无缺失样本总数的比例**。

例子：如下图所示

左侧为无缺失样本的划分结果。
8,10都是存在缺失值的，所以将他们全部划分到三个子节点。
然后以各个子节点无缺失样本占所有无缺失样本的比例作为他们各自的权值。

<img src="https://img2020.cnblogs.com/i-beta/1533981/202003/1533981-20200312154941305-1150347064.png" alt="img" style="zoom:80%;" />

#### 问题3：预测时样本该属性缺失如何处理？

*注意：这里缺失样本6的标签是Play*

<img src="https://img-blog.csdnimg.cn/20181111225608390.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xlYWZfeml6aQ==,size_16,color_FFFFFF,t_70" alt="img" style="zoom: 50%;" />

用前面的计算过程，会得到一个决策树，我们先看一下每个样本在树节点中的分布情况：

<img src="https://img-blog.csdnimg.cn/20181112220934131.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xlYWZfeml6aQ==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:67%;" />

然后我们对树的结构做个改进，给每个叶子节点加个数值，例如（N/E)**，N表示该叶节点中所包含的总样本数**，**E表示与该叶节点的类别不同的样本数**，然后就可以得到这样的树结构。
注意：**缺失值样本6是被划分到与其类别不同的标签下**，即其label是Play，但是会将其归到Don't Play下

<img src="https://img-blog.csdnimg.cn/2018111222412490.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xlYWZfeml6aQ==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:80%;" />

现在我们使用这个决策树对测试样本分类，样本的属性值为：ountlook=sunny, temperature=70, humidity=?, windy=false.

测试样本进入左侧第一个分支，但是humidity属性值未知，因此两个分支的可能性都会考虑：

如果humidity<=75，类别是play。
如果humidity>75，类别是Don‘t play的概率为3/3.4（88%），play的概率是0.4/3.4（12%）。
那么，总的类别分布情况是：
$$
\begin{aligned}
Play:&\frac{2}{5.4}+\frac{3.4}{5.4}*\frac{0.4}{3.4}=44\%\\
Don'tPlay:&\frac{3.4}{5.4}*\frac{3}{3.4}=56\%\\
5.4&是总的样本数=2+3.4
\end{aligned}
$$
因此，该样本的类别判定为Don't play。

###### 总结

就是正常划分，**遇到缺失值对应的属性，就直接计算其在所有子情况下属于各个类别的概率和**，然后取概率最大对应的类别即可。

#### 参考链接 	

[机器学习笔记（7）——C4.5决策树中的缺失值处理](https://blog.csdn.net/leaf_zizi/article/details/83503167)

### xgboost怎么处理缺失值?

xgboost处理缺失值的方法和其他树模型不同。xgboost把缺失值当做稀疏矩阵来对待，本身的在节点分裂时不考虑的缺失值的数值。缺<u>失值数据会被分到左子树和右子树分别计算损失，选择较优的那一个。如果训练中没有数据缺失，预测时出现了数据缺失，那么默认被分类到右子树。</u>
原文链接：https://blog.csdn.net/qq_19446965/article/details/81637199
