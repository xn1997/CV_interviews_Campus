## 剪枝

目的：为了防止模型过拟合

### 预剪枝

（训练时）**构造树时进行剪枝**。

1. 将数据集划分为训练集和验证集
2. **从上往下，**对于一个节点（属性）
   1. 如果划分前准确率低于划分后准确率，那么就使用该属性划分。
   2. 如果划分前准确率不低于（小于等于）划分后准确率，就不就行划分，直接将该节点作为叶子节点。输出类别就是该训练样例中数目占比最多的类。

优点：

1. 很多分支都没有展开（提前结束了节点划分），**降低了过拟合**风险。
2. 也**减少了决策树的训练时间和预测时间**。

缺点：

1. 预剪枝是基于贪心的思想，只考虑当前局部最优，即有些节点虽然当前的划分不能提升泛化性，但在其基础上的后续划分有可能提升性能，这将**导致欠拟合**。

### 后剪枝

**生成决策树之后进行。**

#### C4.5的方法

1. 将数据集划分为训练集和验证集
2. **从下往上**，对于每一个节点
   1. 如果该节点删除后的准确率高于不删除的准确率，那么就删除该属性。
   2. 如果该节点删除后的准确率不高于不删除的的准确率，那么就保留该属性。

优点：

1. 通常比预剪枝保留了更多的分支，**欠拟合风险较小，泛华性能优于预剪枝**。

缺点：

1. 在完成决策树之后进行的，且需要从下往上的对所有非叶子节点进行判断，所以**训练时间大大增加**。

#### CART的方法

采用一种“基于代价复杂度的剪枝”方法进行后剪枝。具体做法为：

我们将一颗充分生长的树称为T0 ，希望减少树的大小来防止过拟化。但同时去掉一些节点后预测的误差可能会增大，那么如何达到这两个变量之间的平衡则是问题的关键。因此我们用一个变量α 来平衡，定义损失函数如下：

![[公式]](https://www.zhihu.com/equation?tex=C_%5Calpha%28T%29%3DC%28T%29%2B%5Calpha%7CT%7C++%5C%5C)

![[公式]](https://www.zhihu.com/equation?tex=T) 为任意子树， ![[公式]](https://www.zhihu.com/equation?tex=C%28T%29) 为预测误差， ![[公式]](https://www.zhihu.com/equation?tex=%7CT%7C) 为子树 ![[公式]](https://www.zhihu.com/equation?tex=T) 的叶子节点个数， ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha) 是参数， ![[公式]](https://www.zhihu.com/equation?tex=C%28T%29) 衡量训练数据的拟合程度， ![[公式]](https://www.zhihu.com/equation?tex=%7CT%7C) 衡量树的复杂度， ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha) 权衡拟合程度与树的复杂度。

那么我们如何找到这个合适的α来使拟合程度与复杂度之间达到最好的平衡呢？准确的方法就是将α从0取到正无穷，对于每一个固定的α，我们都可以找到使得Cα(T)最小的最优子树T(α)

- 当α很小的时候，T0 是这样的最优子树
- 当α很大的时候，单独一个根节点就是最优子树

尽管α的取值无限多，但是T0的子树是有限个。Tn是最后剩下的根结点，子树生成是根据前一个子树Ti，剪掉某个内部节点后，生成$T_{i+1}$。然后**对这样的子树序列分别用测试集进行交叉验证，找到最优的那个子树作为我们的决策树**。因此CART剪枝分为两部分，分别是**生成子树序列**和**交叉验证**。

##### 参考链接

[【预估排序】Xgboost、GBDT、CART等树模型联系和区别（超级详细）](https://zhuanlan.zhihu.com/p/158633779)

### 参考链接

<u>《机器学习》</u>

